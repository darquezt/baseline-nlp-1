{
"cells": [
{
"cell_type": "markdown",
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T19:18:43.301002Z",
"start_time": "2019-08-21T19:18:43.298037Z"
}
},
"source": [
"# Baseline tarea 1\n",
"\n",
"-----------------------------\n",
"\n",
"\n",
"\n",
"\n",
"## Importar librerías y utiles"
]
},
{
"cell_type": "code",
"execution_count": 1,
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T19:45:52.624502Z",
"start_time": "2019-08-21T19:45:48.613907Z"
}
},
"outputs": [],
"source": [
"import pandas as pd\n",
"import shutil\n",
"\n",
"from sklearn.feature_extraction.text import CountVectorizer\n",
"from sklearn.naive_bayes import MultinomialNB\n",
"from sklearn.pipeline import Pipeline\n",
"from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
"from sklearn.model_selection import train_test_split\n",
"\n",
"import os\n",
"import numpy as np"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Datos\n",
"\n",
"### Obtener los datasets desde el github del curso"
]
},
{
"cell_type": "code",
"execution_count": 2,
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T19:45:55.819485Z",
"start_time": "2019-08-21T19:45:52.626520Z"
}
},
"outputs": [],
"source": [
"train = {\n",
"    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
"    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
"    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
"    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
"}\n",
"\n",
"target = {\n",
"    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
"    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
"    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
"    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
"}"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### Analizar los datos \n",
"\n",
"Imprimir la cantidad de tweets de cada dataset, según su intensidad de sentimiento"
]
},
{
"cell_type": "code",
"execution_count": 3,
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T19:45:55.879257Z",
"start_time": "2019-08-21T19:45:55.826364Z"
}
},
"outputs": [
{
"name": "stdout",
"output_type": "stream",
"text": [
"anger \n",
"                       id  tweet  class\n",
"sentiment_intensity                   \n",
"high                 163    163    163\n",
"low                  161    161    161\n",
"medium               617    617    617\n",
"fear \n",
"                       id  tweet  class\n",
"sentiment_intensity                   \n",
"high                 270    270    270\n",
"low                  288    288    288\n",
"medium               699    699    699\n",
"joy \n",
"                       id  tweet  class\n",
"sentiment_intensity                   \n",
"high                 195    195    195\n",
"low                  219    219    219\n",
"medium               488    488    488\n",
"sadness \n",
"                       id  tweet  class\n",
"sentiment_intensity                   \n",
"high                 197    197    197\n",
"low                  210    210    210\n",
"medium               453    453    453\n"
]
}
],
"source": [
"def get_group_dist(group_name, train):\n",
"    print(group_name, \"\\n\",\n",
"          train[group_name].groupby('sentiment_intensity').count())\n",
"\n",
"\n",
"for key in train:\n",
"    get_group_dist(key, train)"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"## Clasificar"
]
},
{
"cell_type": "code",
"execution_count": 4,
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T19:45:55.891188Z",
"start_time": "2019-08-21T19:45:55.882211Z"
}
},
"outputs": [],
"source": [
"# Metrica de evaluación. No tocar...\n",
"def auc(test_set, predicted_set):\n",
"    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
"    medium_predicted = np.array(\n",
"        [prediction[1] for prediction in predicted_set])\n",
"    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
"\n",
"    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
"    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
"    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
"\n",
"    auc_high = roc_auc_score(high_test, high_predicted)\n",
"    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
"    auc_low = roc_auc_score(low_test, low_predicted)\n",
"\n",
"    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
"             high_test.sum() * auc_high) / (\n",
"                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
"    return auc_w    "
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### Dividir el dataset en entrenamiento y prueba"
]
},
{
"cell_type": "code",
"execution_count": 5,
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T19:45:55.901161Z",
"start_time": "2019-08-21T19:45:55.893181Z"
}
},
"outputs": [],
"source": [
"def split_dataset(dataset):\n",
"    # Dividir el dataset en train set y test set\n",
"    X_train, X_test, y_train, y_test = train_test_split(\n",
"        dataset.tweet,\n",
"        dataset.sentiment_intensity,\n",
"        shuffle=True,\n",
"        test_size=0.33)\n",
"    return X_train, X_test, y_train, y_test"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### Definir el clasificador\n",
"\n",
"Consejo para el vectorizador: investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation`. También, el parámetro ngram_range para clasificadores no bayesianos.\n",
"\n",
"Consejo para el clasificador: investigar otros clasificadores mas efectivos que naive bayes. Ojo q naive bayes no debería usarse con n-gramas, ya que rompe el supuesto de independencia.\n"
]
},
{
"cell_type": "code",
"execution_count": 18,
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T19:50:52.114345Z",
"start_time": "2019-08-21T19:50:52.110384Z"
}
},
"outputs": [],
"source": [
"# Definimos el pipeline con el vectorizador y el clasificador.\n",
"def get_classifier():\n",
"     # Inicializamos el Vectorizador para transformar las oraciones a BoW \n",
"    vectorizer = CountVectorizer()\n",
"    \n",
"    # Inicializamos el Clasificador.\n",
"    naive_bayes = MultinomialNB()\n",
"    \n",
"    # Establecer el pipeline.\n",
"    text_clf = Pipeline([('vect', vectorizer), ('clf', naive_bayes)])\n",
"    return text_clf"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### Definir evaluación\n",
"\n",
"Esta función imprime la matriz de confusión, el reporte de clasificación y las metricas usadas en la competencia:\n",
"\n",
"\n",
"- `auc`\n",
"- `kappa`\n",
"- `accuracy`"
]
},
{
"cell_type": "code",
"execution_count": 26,
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T19:53:24.009626Z",
"start_time": "2019-08-21T19:53:24.002646Z"
}
},
"outputs": [],
"source": [
"def evaulate(predicted, y_test, labels):\n",
"    # Importante: al transformar los arreglos de probabilidad a clases,\n",
"    # entregar el arreglo de clases aprendido por el clasificador. \n",
"    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
"    predicted_labels = [labels[np.argmax(item)] for item in predicted]\n",
"    \n",
"    # Confusion Matrix\n",
"    print('Confusion Matrix for {}:\\n'.format(key))\n",
"\n",
"    # Classification Report\n",
"    print(\n",
"        confusion_matrix(y_test,\n",
"                         predicted_labels,\n",
"                         labels=['low', 'medium', 'high']))\n",
"\n",
"    print('\\nClassification Report')\n",
"    print(\n",
"        classification_report(y_test,\n",
"                              predicted_labels,\n",
"                              labels=['low', 'medium', 'high']))\n",
"\n",
"    # AUC\n",
"    print(\"auc: \", auc(y_test, predicted))\n",
"\n",
"    # Kappa\n",
"    print(\"kappa:\", cohen_kappa_score(y_test, predicted_labels))\n",
"\n",
"    # Accuracy\n",
"    print(\"accuracy:\", accuracy_score(y_test, predicted_labels), \"\\n\")\n",
"\n",
"    print('------------------------------------------------------\\n\\n')"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### Ejecutar el clasificador para cierto dataset\n",
"\n",
"Clasifica un dataset. Retorna el modelo ya entrenado mas sus labels asociadas.\n"
]
},
{
"cell_type": "code",
"execution_count": 27,
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T19:53:25.978116Z",
"start_time": "2019-08-21T19:53:25.973129Z"
},
"scrolled": true
},
"outputs": [],
"source": [
"def classify(dataset, key):\n",
"\n",
"    X_train, X_test, y_train, y_test = split_dataset(dataset)\n",
"    text_clf = get_classifier()\n",
"\n",
"    # Entrenar el clasificador\n",
"    text_clf.fit(X_train, y_train)\n",
"\n",
"    # Predecir las probabilidades de intensidad de cada elemento del set de prueba.\n",
"    predicted = text_clf.predict_proba(X_test)\n",
"\n",
"    # Obtener las clases aprendidas.\n",
"    learned_labels = text_clf.classes_\n",
"\n",
"    # Evaluar\n",
"    evaulate(predicted, y_test, learned_labels)\n",
"    return text_clf, learned_labels"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### Ejecutar el clasificador por cada dataset\n"
]
},
{
"cell_type": "code",
"execution_count": 28,
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T19:53:27.106461Z",
"start_time": "2019-08-21T19:53:26.933924Z"
}
},
"outputs": [
{
"name": "stdout",
"output_type": "stream",
"text": [
"Confusion Matrix for anger:\n",
"\n",
"[[  7  38   0]\n",
" [  7 198  10]\n",
" [  0  37  14]]\n",
"\n",
"Classification Report\n",
"              precision    recall  f1-score   support\n",
"\n",
"         low       0.50      0.16      0.24        45\n",
"      medium       0.73      0.92      0.81       215\n",
"        high       0.58      0.27      0.37        51\n",
"\n",
"    accuracy                           0.70       311\n",
"   macro avg       0.60      0.45      0.47       311\n",
"weighted avg       0.67      0.70      0.66       311\n",
"\n",
"auc:  0.4419513084436295\n",
"kappa: 0.20900143757602563\n",
"accuracy: 0.7041800643086816 \n",
"\n",
"------------------------------------------------------\n",
"\n",
"\n",
"Confusion Matrix for fear:\n",
"\n",
"[[ 20  66   5]\n",
" [ 19 195  17]\n",
" [  2  65  26]]\n",
"\n",
"Classification Report\n",
"              precision    recall  f1-score   support\n",
"\n",
"         low       0.49      0.22      0.30        91\n",
"      medium       0.60      0.84      0.70       231\n",
"        high       0.54      0.28      0.37        93\n",
"\n",
"    accuracy                           0.58       415\n",
"   macro avg       0.54      0.45      0.46       415\n",
"weighted avg       0.56      0.58      0.54       415\n",
"\n",
"auc:  0.4351498355972775\n",
"kappa: 0.18612776700779943\n",
"accuracy: 0.5807228915662651 \n",
"\n",
"------------------------------------------------------\n",
"\n",
"\n",
"Confusion Matrix for joy:\n",
"\n",
"[[ 17  61   1]\n",
" [ 15 124  14]\n",
" [  1  41  24]]\n",
"\n",
"Classification Report\n",
"              precision    recall  f1-score   support\n",
"\n",
"         low       0.52      0.22      0.30        79\n",
"      medium       0.55      0.81      0.65       153\n",
"        high       0.62      0.36      0.46        66\n",
"\n",
"    accuracy                           0.55       298\n",
"   macro avg       0.56      0.46      0.47       298\n",
"weighted avg       0.55      0.55      0.52       298\n",
"\n",
"auc:  0.3986003895174781\n",
"kappa: 0.19188500356815175\n",
"accuracy: 0.5536912751677853 \n",
"\n",
"------------------------------------------------------\n",
"\n",
"\n",
"Confusion Matrix for sadness:\n",
"\n",
"[[ 11  48   1]\n",
" [  9 126   6]\n",
" [  0  72  11]]\n",
"\n",
"Classification Report\n",
"              precision    recall  f1-score   support\n",
"\n",
"         low       0.55      0.18      0.27        60\n",
"      medium       0.51      0.89      0.65       141\n",
"        high       0.61      0.13      0.22        83\n",
"\n",
"    accuracy                           0.52       284\n",
"   macro avg       0.56      0.40      0.38       284\n",
"weighted avg       0.55      0.52      0.45       284\n",
"\n",
"auc:  0.45477122221508787\n",
"kappa: 0.10749607172566789\n",
"accuracy: 0.5211267605633803 \n",
"\n",
"------------------------------------------------------\n",
"\n",
"\n"
]
}
],
"source": [
"classifiers = []\n",
"learned_labels_array = []\n",
"\n",
"# Por cada llave en train ('anger', 'fear', 'joy', 'sadness')\n",
"for key in train:\n",
"    classifier, learned_labels = classify(train[key], key)\n",
"    classifiers.append(classifier)\n",
"    learned_labels_array.append(learned_labels)"
]
},
{
"cell_type": "markdown",
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T19:37:43.169737Z",
"start_time": "2019-08-21T19:37:43.166744Z"
}
},
"source": [
"## Predecir target set"
]
},
{
"cell_type": "code",
"execution_count": 22,
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T19:50:59.474909Z",
"start_time": "2019-08-21T19:50:59.469921Z"
}
},
"outputs": [],
"source": [
"def predict_target(dataset, classifier, labels):\n",
"    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
"    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
"    # Agregar ids\n",
"    predicted['id'] = dataset.id.values\n",
"    # Reordenar\n",
"    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
"    return predicted"
]
},
{
"cell_type": "markdown",
"metadata": {},
"source": [
"### Ejecutar la predicción y guardar archivos."
]
},
{
"cell_type": "code",
"execution_count": 33,
"metadata": {
"ExecuteTime": {
"end_time": "2019-08-21T20:00:10.724762Z",
"start_time": "2019-08-21T20:00:10.576665Z"
},
"scrolled": true
},
"outputs": [],
"source": [
"predicted_target = {}\n",
"\n",
"if (not os.path.isdir('./predictions')):\n",
"    os.mkdir('./predictions')\n",
"\n",
"else:\n",
"    # Eliminar predicciones anteriores:\n",
"    shutil.rmtree('./predictions')\n",
"    os.mkdir('./predictions')\n",
"\n",
"for idx, key in enumerate(target):\n",
"    # Predecir el target set\n",
"    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
"                                           learned_labels_array[idx])\n",
"    # Guardar predicciones\n",
"    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
"                                 sep='\\t',\n",
"                                 header=False,\n",
"                                 index=False)\n",
"\n",
"# Crear archivo zip\n",
"a = shutil.make_archive('predictions', 'zip', './predictions')"
]
}
],
"metadata": {
"kernelspec": {
"display_name": "Python 3",
"language": "python",
"name": "python3"
},
"language_info": {
"codemirror_mode": {
"name": "ipython",
"version": 3
},
"file_extension": ".py",
"mimetype": "text/x-python",
"name": "python",
"nbconvert_exporter": "python",
"pygments_lexer": "ipython3",
"version": "3.7.3"
},
"toc": {
"base_numbering": 1,
"nav_menu": {},
"number_sections": true,
"sideBar": true,
"skip_h1_title": true,
"title_cell": "Table of Contents",
"title_sidebar": "Contents",
"toc_cell": true,
"toc_position": {},
"toc_section_display": true,
"toc_window_display": false
},
"varInspector": {
"cols": {
"lenName": 16,
"lenType": 16,
"lenVar": 40
},
"kernels_config": {
"python": {
"delete_cmd_postfix": "",
"delete_cmd_prefix": "del ",
"library": "var_list.py",
"varRefreshCmd": "print(var_dic_list())"
},
"r": {
"delete_cmd_postfix": ") ",
"delete_cmd_prefix": "rm(",
"library": "var_list.r",
"varRefreshCmd": "cat(var_dic_list()) "
}
},
"types_to_exclude": [
"module",
"function",
"builtin_function_or_method",
"instance",
"_Feature"
],
"window_display": false
}
},
"nbformat": 4,
"nbformat_minor": 2
}
